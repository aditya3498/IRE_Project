{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Startd\n",
      "https://t.co/XWFUWbD9bm\n",
      "It must be said that Morey and the Americans never dreamed that the people he supported supported racial discrimination.\n",
      "#NBA #HongKonazi #BlackLivesMatter \n",
      "#HongKong #LeBronJames https://t.co/72MtrAMtpF\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "parse() missing 2 required positional arguments: 'api' and 'json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d72453b6b22e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tweet Ended\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: parse() missing 2 required positional arguments: 'api' and 'json'"
     ]
    }
   ],
   "source": [
    "####input your credentials here\n",
    "consumer_key = '95cMtk1vJvEEW2rlMR0kIU9lE'\n",
    "consumer_secret = 'pMQFi7LBdcudKDNZOokUJGS8mDxQanUv8spxBDdTLiwSZBuUOM'\n",
    "access_token = '1036313393114767360-BZ8Qpi02ghRvehhcITEIyl7SmGWmU6'\n",
    "access_token_secret = 'C7VAqGDhTdB424iBtEwF1CJI9YPTcvNvLjFmaCXENNv3G'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "#####United Airlines\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('ua.csv', 'a')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for status in tweepy.Cursor(api.search,q=\"#blacklivesmatter\",count=1,tweet_mode = 'extended',\n",
    "                           lang=\"en\",\n",
    "                           since=\"2017-04-03\").items():\n",
    "    print(\"Tweet Startd\")\n",
    "    if hasattr(status, \"retweeted_status\"):  # Check if Retweet\n",
    "        try:\n",
    "            print(status.retweeted_status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(status.retweeted_status.full_text)\n",
    "    else:\n",
    "        try:\n",
    "            print(status.extended_tweet[\"full_text\"])\n",
    "        except AttributeError:\n",
    "            print(status.full_text)\n",
    "    #print(status.parse())\n",
    "    print(\"Tweet Ended\")\n",
    "    #csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet():\n",
    "    \n",
    "    def __init__(self, status_response):\n",
    "        self.id = status_response.id\n",
    "        self._get_text(status_response)\n",
    "        self.user_info = status_response.user\n",
    "        self.entitites = status_response.entities\n",
    "        \n",
    "    def _get_text(self, status):\n",
    "        if hasattr(status, \"retweeted_status\"):  # Check if Retweet\n",
    "            try:\n",
    "                self.text = status.retweeted_status.extended_tweet[\"full_text\"]\n",
    "            except AttributeError:\n",
    "                self.text = status.retweeted_status.full_text\n",
    "        else:\n",
    "            try:\n",
    "                self.text = status.extended_tweet[\"full_text\"]\n",
    "            except AttributeError:\n",
    "                self.text = status.full_text\n",
    "    \n",
    "    #code for generating named entities of each tweet\n",
    "    def _get_named_entities(self):\n",
    "        pass\n",
    "    #code for generating event phrases\n",
    "    def _get_event_phrases(self):\n",
    "        pass\n",
    "    \n",
    "    def get_graph_entities(self):\n",
    "        return self._get_named_entities() + self._get_event_phrases()\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.id\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "    \n",
    "class TweetRetriever():\n",
    "\n",
    "    def __init__(self):\n",
    "        consumer_key = '95cMtk1vJvEEW2rlMR0kIU9lE'\n",
    "        consumer_secret = 'pMQFi7LBdcudKDNZOokUJGS8mDxQanUv8spxBDdTLiwSZBuUOM'\n",
    "        access_token = '1036313393114767360-BZ8Qpi02ghRvehhcITEIyl7SmGWmU6'\n",
    "        access_token_secret = 'C7VAqGDhTdB424iBtEwF1CJI9YPTcvNvLjFmaCXENNv3G'\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_token_secret)\n",
    "        self.api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    \n",
    "    def getTweets(self, hashtag, count = 10):\n",
    "        tweets = []\n",
    "        for status in tweepy.Cursor(self.api.search, q = hashtag, count = count, \n",
    "                                    tweet_mode = 'extended', lang = 'en').items():\n",
    "            tweets.append(Tweet(status))\n",
    "        return tweets\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode():\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tweeets = set()\n",
    "        self.value = 0\n",
    "        \n",
    "    def add_tweet(self, tweet):\n",
    "        self.tweeets.add(tweet)\n",
    "    \n",
    "    def common_tweets(self, other):\n",
    "        return len(self.tweeets.intersection(other.tweets))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.name\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "    \n",
    "class TweetGraph():\n",
    "    \n",
    "    def __init__(self, topic):\n",
    "        self.topic = topic\n",
    "        self.nodes = {}\n",
    "        self.edge_map = {}\n",
    "        \n",
    "    def add_entity(self, name, tweet_ref):\n",
    "        if name not in nodes:\n",
    "            nodes[name] = GraphNode(name)\n",
    "        nodes[name].add_tweet(tweet_ref)\n",
    "    \n",
    "    def add_edge(self, node1, node2):\n",
    "        assert node1.name in self.node_map\n",
    "        assert node2.name in self.node_map\n",
    "        weight = node1.common_tweets(node2)\n",
    "        self.edge_map.setdefault(node1.name, {}).setdefault(node2.name, weight)\n",
    "        self.edge_map.setdefault(node2.name, {}).setdefault(node1.name, weight)\n",
    "    \n",
    "    def compute_all_edges(self):\n",
    "        for node1 in self.nodes.values():\n",
    "            for node2 in self.nodes.values():\n",
    "                self.add_edge(node1, node2)\n",
    "    \n",
    "    def _get_pagerank_matrix(self):\n",
    "        x = [[0 for _ in len(self.nodes)] for _ in len(self.nodes)]\n",
    "        for i, node1 in enumerate(self.nodes.values()):\n",
    "            wsum = 0\n",
    "            for node2 in self.nodes.values():\n",
    "                wsum += self.edge_map.get(node1.name, {}).get(node2.name, 0)\n",
    "            for j, node2 in enumerate(self.nodes.values()):\n",
    "                x[i][j] = self.edge_map.get(node1.name, {}).get(node2.name, 0)/wsum\n",
    "        return np.array(x)\n",
    "    \n",
    "    def set_textrank_values(self, d = 0.85):\n",
    "        rank_graph = nx.from_numpy_array(self._get_pagerank_matrix())\n",
    "        node_scores = nx.pagerank(rank_graph, aplpha = d)\n",
    "        for i, node in enumerate(self.nodes.values()):\n",
    "            node.value = node_scores[i]\n",
    "    \n",
    "    def get_weight(self, node1, node2):\n",
    "        return self.edge_map(node1.name, {}).get(node2.name, 0)\n",
    "    \n",
    "    def get_topic_similarity(self, node):\n",
    "        if node.name in self.topic:\n",
    "            return len(node.tweeets)\n",
    "        return 1\n",
    "    \n",
    "    def get_nodes_above_thres(self, thres = 1):\n",
    "        nodes = []\n",
    "        value_sum = 0\n",
    "        for node in self.nodes.values():\n",
    "            if node.value > thres:\n",
    "                nodes.append(node)\n",
    "                value_sum += node.value\n",
    "        return nodes, value_sum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGraph(topic, tweets):\n",
    "    tweetGraph = TweetGraph(topic)\n",
    "    for tweet in tweets:\n",
    "        graph_entities = tweet.get_graph_entities()\n",
    "        for name in graph_entities:\n",
    "            tweetGraph.add_entity(name, tweet)\n",
    "    tweetGraph.compute_all_edges()\n",
    "    tweetGraph.set_textrank_values()\n",
    "    return tweetGraph\n",
    "\n",
    "def partitionGraph(tweetGraph, alpha, beta, high_rank_thres = 1):\n",
    "    \n",
    "    #initialize highly ranked nodes and their total values sum\n",
    "    high_ranked_nodes, total_value_sum = tweetGraph.get_nodes_above_thres(high_rank_thres)\n",
    "    high_ranked_nodes = sorted(high_ranked_nodes, key = lambda x: x.value)\n",
    "    partitions = []\n",
    "    \n",
    "    #partitioning loop\n",
    "    while len(high_ranked_nodes):\n",
    "        #entity set is the nodes in the partition\n",
    "        entity_set = set()\n",
    "        repr_node = high_ranked_nodes.pop()\n",
    "        entitiy_set.add(repr_node)\n",
    "        repr_node_topic_similarity = tweetGraph.get_topic_similarity(repr_node)\n",
    "        value_sum = repr_node.value\n",
    "        \n",
    "        for node in high_ranked_nodes:\n",
    "            node_edge_weight = tweetGraph.get_weight(repr_node, node)\n",
    "            node_topic_similarity = tweetGraph.get_topic_similarity(node)\n",
    "            \n",
    "            if node_edge_weight/repr_node_topic_similarity > alpha and \\\n",
    "                node_topic_similarity/repr_node_topic_similarity > alpha:\n",
    "                entity_set.add(node)\n",
    "                value_sum += node.value\n",
    "        \n",
    "        if value_sum/total_value_sum > beta:\n",
    "            temp = []\n",
    "            for node in high_ranked_nodes:\n",
    "                if node not in entitiy_set:\n",
    "                    temp.append(node)\n",
    "            high_ranked_nodes = temp\n",
    "            \n",
    "        partitions.append([])\n",
    "        for node in list(entity_set):\n",
    "            partitions[-1].append(node)\n",
    "            \n",
    "    return partitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = TweetRetriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = ret.getTweets('#worlds2019', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@TeamGriffinLoL vs @G2esports tiebreaker highlights\\U0001f973:\\n\\n3 min GRF FB\\n18 min GRF 3 for 0 mid lane\\n21 min GRF 4 for 2 blue side jungle\\n27 min GRF ACE in base to win\\n\\n#lckwin #Worlds2019'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1].text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
